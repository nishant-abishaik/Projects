{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Creating a logistic regression to predict absenteeism"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Import the relevant libraries"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import the relevant libraries\r\n",
    "import pandas as pd\r\n",
    "import numpy as np"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# load the preprocessed CSV data\r\n",
    "data_preprocessed = pd.read_csv('Preprocessed_Data\\Absenteeism_preprocessed.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# eyeball the data\r\n",
    "data_preprocessed.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create the targets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# find the median of 'Absenteeism Time in Hours'\r\n",
    "data_preprocessed['Absenteeism Time in Hours'].median()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create targets for our logistic regression\r\n",
    "# they have to be categories and we must find a way to say if someone is 'being absent too much' or not\r\n",
    "# what we've decided to do is to take the median of the dataset as a cut-off line\r\n",
    "# in this way the dataset will be balanced (there will be roughly equal number of 0s and 1s for the logistic regression)\r\n",
    "# as balancing is a great problem for ML, this will work great for us\r\n",
    "# alternatively, if we had more data, we could have found other ways to deal with the issue \r\n",
    "# for instance, we could have assigned some arbitrary value as a cut-off line, instead of the median\r\n",
    "\r\n",
    "# note that what line does is to assign 1 to anyone who has been absent 4 hours or more (more than 3 hours)\r\n",
    "# that is the equivalent of taking half a day off\r\n",
    "\r\n",
    "# initial code from the lecture\r\n",
    "# targets = np.where(data_preprocessed['Absenteeism Time in Hours'] > 3, 1, 0)\r\n",
    "\r\n",
    "# parameterized code\r\n",
    "targets = np.where(data_preprocessed['Absenteeism Time in Hours'] > \r\n",
    "                   data_preprocessed['Absenteeism Time in Hours'].median(), 1, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# eyeball the targets\r\n",
    "targets"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create a Series in the original data frame that will contain the targets for the regression\r\n",
    "data_preprocessed['Excessive Absenteeism'] = targets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check what happened\n",
    "# maybe manually see how the targets were created\n",
    "data_preprocessed.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A comment on the targets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check if dataset is balanced (what % of targets are 1s)\n",
    "# targets.sum() will give us the number of 1s that there are\n",
    "# the shape[0] will give us the length of the targets array\n",
    "targets.sum() / targets.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create a checkpoint by dropping the unnecessary variables\n",
    "# also drop the variables we 'eliminated' after exploring the weights\n",
    "data_with_targets = data_preprocessed.drop(['Absenteeism Time in Hours'],axis=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check if the line above is a checkpoint :)\n",
    "\n",
    "# if data_with_targets is data_preprocessed = True, then the two are pointing to the same object\n",
    "# if it is False, then the two variables are completely different and this is in fact a checkpoint\n",
    "data_with_targets is data_preprocessed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check what's inside\n",
    "data_with_targets.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Select the inputs for the regression"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_with_targets.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Selects all rows and all columns until 14 (excluding)\n",
    "data_with_targets.iloc[:,:14]"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Selects all rows and all columns but the last one (basically the same operation)\n",
    "data_with_targets.iloc[:,:-1]"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a variable that will contain the inputs (everything without the targets)\n",
    "unscaled_inputs = data_with_targets.iloc[:,:-1]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Standardize the data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# standardize the inputs\n",
    "\n",
    "# standardization is one of the most common preprocessing tools\n",
    "# since data of different magnitude (scale) can be biased towards high values,\n",
    "# we want all inputs to be of similar magnitude\n",
    "# this is a peculiarity of machine learning in general - most (but not all) algorithms do badly with unscaled data\n",
    "\n",
    "# a very useful module we can use is StandardScaler \n",
    "# it has much more capabilities than the straightforward 'preprocessing' method\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# we will create a variable that will contain the scaling information for this particular dataset\n",
    "# here's the full documentation: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html\n",
    "\n",
    "# define scaler as an object\n",
    "absenteeism_scaler = StandardScaler()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import the libraries needed to create the Custom Scaler\n",
    "# note that all of them are a part of the sklearn package\n",
    "# moreover, one of them is actually the StandardScaler module, \n",
    "# so you can imagine that the Custom Scaler is build on it\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# create the Custom Scaler class\n",
    "\n",
    "class CustomScaler(BaseEstimator,TransformerMixin): \n",
    "    \n",
    "    # init or what information we need to declare a CustomScaler object\n",
    "    # and what is calculated/declared as we do\n",
    "    \n",
    "    def __init__(self,columns,copy=True,with_mean=True,with_std=True):\n",
    "        \n",
    "        # scaler is nothing but a Standard Scaler object\n",
    "        self.scaler = StandardScaler(copy,with_mean,with_std)\n",
    "        # with some columns 'twist'\n",
    "        self.columns = columns\n",
    "        self.mean_ = None\n",
    "        self.var_ = None\n",
    "        \n",
    "    \n",
    "    # the fit method, which, again based on StandardScale\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.scaler.fit(X[self.columns], y)\n",
    "        self.mean_ = np.mean(X[self.columns])\n",
    "        self.var_ = np.var(X[self.columns])\n",
    "        return self\n",
    "    \n",
    "    # the transform method which does the actual scaling\n",
    "\n",
    "    def transform(self, X, y=None, copy=None):\n",
    "        \n",
    "        # record the initial order of the columns\n",
    "        init_col_order = X.columns\n",
    "        \n",
    "        # scale all features that you chose when creating the instance of the class\n",
    "        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n",
    "        \n",
    "        # declare a variable containing all information that was not scaled\n",
    "        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n",
    "        \n",
    "        # return a data frame which contains all scaled features and all 'not scaled' features\n",
    "        # use the original order (that you recorded in the beginning)\n",
    "        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check what are all columns that we've got\n",
    "unscaled_inputs.columns.values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# choose the columns to scale\n",
    "# we later augmented this code and put it in comments\n",
    "# columns_to_scale = ['Month Value','Day of the Week', 'Transportation Expense', 'Distance to Work',\n",
    "       #'Age', 'Daily Work Load Average', 'Body Mass Index', 'Children', 'Pet']\n",
    "    \n",
    "# select the columns to omit\n",
    "columns_to_omit = ['Reason_1', 'Reason_2', 'Reason_3', 'Reason_4','Education']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create the columns to scale, based on the columns to omit\n",
    "# use list comprehension to iterate over the list\n",
    "columns_to_scale = [x for x in unscaled_inputs.columns.values if x not in columns_to_omit]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# declare a scaler object, specifying the columns you want to scale\n",
    "absenteeism_scaler = CustomScaler(columns_to_scale)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fit the data (calculate mean and standard deviation); they are automatically stored inside the object \n",
    "absenteeism_scaler.fit(unscaled_inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# standardizes the data, using the transform method \n",
    "# in the last line, we fitted the data - in other words\n",
    "# we found the internal parameters of a model that will be used to transform data. \n",
    "# transforming applies these parameters to our data\n",
    "# note that when you get new data, you can just call 'scaler' again and transform it in the same way as now\n",
    "scaled_inputs = absenteeism_scaler.transform(unscaled_inputs)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# the scaled_inputs are now an ndarray, because sklearn works with ndarrays\n",
    "scaled_inputs"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check the shape of the inputs\n",
    "scaled_inputs.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split the data into train & test and shuffle"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import the relevant module"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import train_test_split so we can split our data into train and test\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check how this method works\n",
    "train_test_split(scaled_inputs, targets)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# declare 4 variables for the split\n",
    "x_train, x_test, y_train, y_test = train_test_split(scaled_inputs, targets, #train_size = 0.8, \n",
    "                                                                            test_size = 0.2, random_state = 20)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check the shape of the train inputs and targets\n",
    "print (x_train.shape, y_train.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check the shape of the test inputs and targets\n",
    "print (x_test.shape, y_test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Logistic regression with sklearn"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# import the LogReg model from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# import the 'metrics' module, which includes important metrics we may want to use\n",
    "from sklearn import metrics"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training the model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create a logistic regression object\n",
    "reg = LogisticRegression()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# fit our train inputs\n",
    "# that is basically the whole training part of the machine learning\n",
    "reg.fit(x_train,y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# assess the train accuracy of the model\n",
    "reg.score(x_train,y_train)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Manually check the accuracy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# find the model outputs according to our model\n",
    "model_outputs = reg.predict(x_train)\n",
    "model_outputs"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compare them with the targets\n",
    "y_train"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# ACTUALLY compare the two variables\n",
    "model_outputs == y_train"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# find out in how many instances we predicted correctly\n",
    "np.sum((model_outputs==y_train))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get the total number of instances\n",
    "model_outputs.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# calculate the accuracy of the model\n",
    "np.sum((model_outputs==y_train)) / model_outputs.shape[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Finding the intercept and coefficients"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get the intercept (bias) of our model\n",
    "reg.intercept_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# get the coefficients (weights) of our model\n",
    "reg.coef_"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# check what were the names of our columns\n",
    "unscaled_inputs.columns.values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# save the names of the columns in an ad-hoc variable\n",
    "feature_name = unscaled_inputs.columns.values"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# use the coefficients from this table (they will be exported later and will be used in Tableau)\n",
    "# transpose the model coefficients (model.coef_) and throws them into a df (a vertical organization, so that they can be\n",
    "# multiplied by certain matrices later) \n",
    "summary_table = pd.DataFrame (columns=['Feature name'], data = feature_name)\n",
    "\n",
    "# add the coefficient values to the summary table\n",
    "summary_table['Coefficient'] = np.transpose(reg.coef_)\n",
    "\n",
    "# display the summary table\n",
    "summary_table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# do a little Python trick to move the intercept to the top of the summary table\n",
    "# move all indices by 1\n",
    "summary_table.index = summary_table.index + 1\n",
    "\n",
    "# add the intercept at index 0\n",
    "summary_table.loc[0] = ['Intercept', reg.intercept_[0]]\n",
    "\n",
    "# sort the df by index\n",
    "summary_table = summary_table.sort_index()\n",
    "summary_table"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Interpreting the coefficients"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# create a new Series called: 'Odds ratio' which will show the.. odds ratio of each feature\n",
    "summary_table['Odds_ratio'] = np.exp(summary_table.Coefficient)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# display the df\n",
    "summary_table"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# sort the table according to odds ratio\n",
    "# note that by default, the sort_values method sorts values by 'ascending'\n",
    "summary_table.sort_values('Odds_ratio', ascending=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}